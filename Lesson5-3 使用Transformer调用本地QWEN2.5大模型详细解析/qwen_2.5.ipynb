{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    # device = \"cuda\"  # 将模型加载到 GPU 上\n",
    "    device = \"cuda:0\"  # 将模型加载到指定GPU 上\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        # device_map=\"auto\"\n",
    "        device_map={\"\": 0}  # 指定模型加载到 GPU 上\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)#加载分词器\n",
    "    return device, tokenizer, model\n",
    "\n",
    "# 定义一个名为 chat_qwen 的函数，用于与模型进行交互。\n",
    "def chat_qwen(device, tokenizer, model, prompt):\n",
    "    messages = (\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    )\n",
    "    # 使用分词器的 apply_chat_template 方法将消息格式化为模型可理解的输入格式\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer((text), return_tensors=\"pt\").to(device)\n",
    "    #生成模型输出\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    # 由于模型输出包括输入模型，这里切去输入部分\n",
    "    generated_ids = (output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids))\n",
    "    # 将模型输出解码为文本\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(response) \n",
    "\n",
    "# 载入模型\n",
    "model_path = './qwen/Qwen2___5-7B-Instruct'\n",
    "device, tokenizer, model = load_model(model_path) \n",
    "\n",
    "# 进行测试\n",
    "chat_qwen(device, tokenizer, model,'你好，请你介绍一下自己') \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
