# Lesson2 基于Pytorch手把手从头实现Transformer

1. 本节课是代码课程。使用Pytorch从零开始实现一个Transformer结构，并完成一个把英文翻译为中文任务，配套视频课程在B站；

2. 本课代码主要来源于：https://github.com/XiaoyongW/transformer_learning。在作者项目的基础上：我们主要修改：

   1. 补充了可供训练的数据集；
   2. 源代码是在CPU上运行，考虑到运行速度，笔者修改为在GPU上运行版本。如你需要在CPU上运行可自行修改，或使用：https://github.com/XiaoyongW/transformer_learning；
   3. 为配合视频课程内容，补充即修改了大量的讲解注释；

3. Lesson2与Lesson1共用讲义链接，讲义链接：https://github.com/haobinlaosi/LLM_Course/tree/main/Lesson1%20Attention-transformer-BERT-GPT

   

