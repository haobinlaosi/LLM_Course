# 大语言模型

我们是暨南大学附属广东省第二人民医院的人工智能研究所的医疗大模型组，主要探索医疗人工智能，医疗大模型的相关工作。

这里是我们组的大语言模型课程，主要讲解大语言模型的原理、相关技术发展。



本课程配套视频课程、讲义链接以及代码实现，一般每周更新一次，欢迎大家star⭐⭐⭐~



|                     课程                      | 内容简介                                                     | 视频课程链接                                                 | 内容链接                                                     | 备注                                                         |
| :-------------------------------------------: | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
|    Lesson1: Attention-transformer-BERT-GPT    | 本节课程主要从Attention机制开始，之后详细介绍Transformer每个模块的组成以及对应的公式。在介绍Transformer后，将分别从两个分支分别介绍其中的编码器发展部分BERT以及解码器发展部分GPT，总共时长约90min。 | [90分钟串讲Attention-Transformer-BERT-GPT](https://www.bilibili.com/video/BV1vwpAeUEaV/?spm_id_from=333.999.0.0&vd_source=eca3715b13ad48cd6f60839e909c5a0d) | [Lesson1: Attention-transformer-BERT-GPT](https://github.com/haobinlaosi/LLM_Course/tree/main/Lesson1%20Attention-transformer-BERT-GPT) | 20240904第一版本上传；<br/>20240912更新对应论文中的对应实现细节； |
| Lesson2: 基于Pytorch手把手从头实现Transformer | 本节课是代码课程。使用Pytorch从零开始实现一个Transformer结构，并完成一个把英文翻译为中文任务。 | [L2：基于Pytorch手把手从头实现Transformer](https://www.bilibili.com/video/BV15RtKeBEwS/?vd_source=eca3715b13ad48cd6f60839e909c5a0d) | [Lesson2 基于Pytorch手把手从头实现Transformer](https://github.com/haobinlaosi/LLM_Course/tree/main/Lesson2%20%20%E5%9F%BA%E4%BA%8EPytorch%E6%89%8B%E6%8A%8A%E6%89%8B%E4%BB%8E%E5%A4%B4%E5%AE%9E%E7%8E%B0Transformer) | Lesson2与Lesson1共用讲义；                                   |
|                                               |                                                              |                                                              |                                                              |                                                              |

